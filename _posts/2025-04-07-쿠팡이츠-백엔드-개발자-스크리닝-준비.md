---
title: "쿠팡 Senior Backend Engineering (Eats Customer) 스크리닝 준비"
categories:
  - junk1
toc: true
toc_sticky: true
---
  
# 💧 전직장 퇴사 이유?
> 저의 개발자로서의 강점은 현업과 소통하며 문제를 해결하고 팀과 협력하려는 과정에서 더 나은 결과를 도출해 낼 수 있다고 생각합니다.
> 하지만 전 직장에서는 독립적인 업무가 많아 기획자 및 개발자와 협력할 기회가 적어 제 강점을 충분히 발휘할 기회가 적었다고 생각합니다.
> 그래서 제 강점을 잘 활용하고 더 넓은 경험을 쌓을 수 있는 환경에서 성장하고 싶어 퇴사를 결심하였습니다.   
> 또한, 약 9년동안 한번도 쉬지 않고 계속 일을 해왔기 때문에 이번에는 잠시 휴식을 취하며 제 커리어와 앞으로의 방향을 차분히 고민할 시간을 가지려고 합니다.

1. 전 직장에서 독립적인 업무가 많았다고 하셨는데, 그렇다면 그곳에서 협력하는 기회가 전혀 없었나요? 혹시 협업을 강화하기 위한 노력을 하지 않으셨나요?
> 독립적인 업무가 많았다 하더라도, 팀 내에서 협업을 강화할 수 있는 방법을 찾을 수 있었다는 점을 언급하면서, 그럼에도 불구하고 지속적으로 협업이 부족했다고 느꼈던 이유를 설명할 수 있습니다. 예를 들어, 협업을 시도했으나 구조적으로나 문화적으로 협업이 어려웠다는 점을 부드럽게 설명하는 것이 중요합니다.

2. 그렇다면 협업의 기회를 늘리기 위해 어떤 방식으로 기획자나 개발자와 소통하려고 시도했나요? 예를 들어, 소통 채널을 더 적극적으로 활용했거나, 자주 미팅을 주도하려는 노력을 하지 않으셨나요?
> 이 질문에 대해서는 적극적으로 소통을 시도했으나, 그 시도가 원하는 결과를 얻지 못했다고 말할 수 있습니다. 예를 들어, "정기적인 미팅을 제안하거나, 의견을 나누기 위해 노력을 했지만, 조직의 업무 문화나 환경상 한계가 있었다"고 솔직하게 설명하고, 그럼에도 불구하고 협업의 중요성을 깨닫고 더 넓은 협업 환경에서 일하고자 결심했다는 점을 강조하는 것이 좋습니다.

3. 그럼에도 불구하고 퇴사 후 휴식을 취하고 싶다고 하셨는데, 이는 단순히 잠시 쉬고 싶다는 것 외에 향후 커리어 방향에 대해 어떤 고민이 있었던 건가요?
> "9년간의 지속적인 경력을 통해 많은 것을 배웠고, 이제는 좀 더 전략적으로 커리어를 계획하고 싶었습니다. 긴 시간 동안 일을 해왔기 때문에 잠시 쉬면서 제 자신의 업무 스타일과 방향성에 대해 다시 한번 고민하고 싶었고, 이를 통해 좀 더 발전할 수 있을 것이라 생각했습니다"라는 식으로 대답할 수 있습니다.   
> "저는 9년간 지속적으로 일을 해오면서 직무에 대한 전문성을 쌓았지만, 한편으로는 제 경력의 방향에 대해 깊이 고민해볼 시간이 필요했습니다. 그래서 퇴사 후 한동안 휴식을 취하며 제 커리어와 향후 방향에 대해 차분히 생각하고, 이를 바탕으로 제 경험을 더 잘 활용할 수 있는 직장을 찾고 있습니다."

---

# 💻 기술/프로젝트 관련 질문
## 1번 질문
Kafka를 활용한 메시징 시스템 개발 경험이 있다고 하셨는데, Kafka를 선택한 이유는 무엇이며, 설계 시 가장 중점을 둔 부분은 무엇이었나요?
> Kafka는 회사에서 이미 사용하고 있어서 자연스럽게 사용하게 되었는데요, 
> 그 안에서 운영 이슈를 직접 해결해보면서 많이 배울 수 있었습니다.
> 초반엔 shopNo % 파티션 수 방식으로 파티션을 할당했었는데, 특정 상점에서 메시지가 몰릴 경우 해당 파티션이 병목이 되어버리고, 
> 같은 파티션을 공유하는 다른 상점들까지 소비가 느려지는 문제가 있었습니다.
> 이 문제를 해결하기 위해 라운드로빈 방식으로 파티션을 분산시키는 구조로 바꿨습니다. 
> Kafka는 partition을 명시하지 않으면 내부적으로 순차적으로 파티션을 배정해주기 때문에, 
> 기존 로직에서 shopNo 기반의 파티션 할당만 제거해주는 방식으로 쉽게 적용할 수 있었고요.
> 그리고 메시지 유실에 대비해서는 웹훅 알림을 통해 유실 메시지 내용을 바로 받을 수 있게 했고, 
> 이 데이터를 다시 발행할 수 있는 관리자 기능을 개발해서 신뢰성을 보완했습니다.
> 운영 측면에선 SRE팀이 구축한 Grafana를 활용해 소비 속도나 장애 징후를 체크했고요. 
> 이런 경험을 통해 Kafka의 구조나 운영 이슈 대응에 대해 실전에서 많이 익혔다고 생각합니다.

### 🎯 1차 질문
Kafka에서 shopNo를 기준으로 파티션 할당했다고 하셨는데, 이 구조에서 어떤 병목 이슈가 발생했는지 구체적으로 설명해주실 수 있을까요?   
예를 들어, 어떤 상황에서 소비가 느려졌는지, 어떤 파티션이 병목이 되었는지 등.
> 상품이미지를 클라우드에 저장하는 방식을 카프카를 통해서 진행하였는데,
> 만약 특정 상점번호가 만개의 상품을 한번에 등록하는 경우 하나의 파티션에 만개 ~ 최소는 12만개 메세지가 발행되게 됩니다. 
> 이런 경우 그 파티션을 쓰는 다른 상점번호는 위 상점이 끝날때까지 상품이미지가 클라우드에 저장되지 않는 이슈가 발생되었습니다.

### 🎯 2차 Kafka 꼬리 질문
그 병목 문제를 해결하기 위해 라운드로빈 방식으로 변경하셨다고 했는데,   
이 구조로 바꾸면서 메시지 순서가 꼬이거나, 다른 부작용은 없었는지 궁금합니다.   
예를 들어, 같은 상점의 메시지가 서로 다른 파티션으로 들어가면 문제가 되지 않았나요?
> 상품을 새로 등록하는 경우에는 처리되는 순서에 의미가 없기 때문에 라운드로빈 방식이 적절하나,
> 상품이미지를 삭제하고 수정하는 경우에는 처리되는 순서가 의미가 있기 때문에 라운드로빈 방식이 적절하지 못하였습니다.
> 그래서 상품이미지를 "새로" 등록하는 경우에만 라운드로빈 방식을 적용하여 병목현상을 해결하였습니다.

### 🎯 3차 Kafka 꼬리 질문
그러면 상품 등록 메시지만 라운드로빈으로 발행하려면, 프로듀서 단에서 어떤 방식으로 구현하셨나요?   
파티션 번호를 아예 지정하지 않은 방식인지, 아니면 특정 로직으로 나눠서 보낸 건지 궁금합니다.
> 기본적으로는 shopNo % partition 수 로 파티션을 고정하는 구조였지만, 
> 상품 등록 메시지의 경우 순서가 중요하지 않아서 성능 병목을 줄이기 위해 파티션 번호를 명시하지 않는 방식으로 수정했습니다.
> Kafka에서는 프로듀서가 파티션을 명시하지 않으면, 기본적으로 라운드로빈 방식으로 파티션이 할당되기 때문에, 
> 등록 메시지에 대해서는 해당 부분의 코드를 주석 처리해서 라운드로빈 방식으로 전송되도록 했습니다.
> 반면 이미지 수정이나 삭제 같은 작업은 순서가 중요했기 때문에 기존의 shopNo 해시 기반 방식으로 유지했습니다.

### 🎯 4차 Kafka 꼬리 질문
메시지 유실 방지를 위해 관리자 기능도 개발하셨다고 하셨는데,   
어떤 기준으로 유실 여부를 판단했고, 어떻게 재발행할 수 있도록 구현하셨나요?
> 메세지 소비가 실패하는 경우 리트라이 토픽을 재발행해 실패되는 메세지가 없도록 작업해놓았습니다. 
> 메세지 소비가 완료되면 데이터베이스의 컬럼값을 완료로 업데이트 해주는데, 
> 스케줄러를 통해서 메세지는 발행됬는데 완료상태로 업데이트 되지 않는 값들을 웹훅 알림으로 받아 
> 관리자가 한꺼번에 다시 재발행할 수 있도록 기능을 작업해놓았습니다.

### 🎯 5차 Kafka 꼬리 질문
메시지 재발행 시 중복 처리 문제는 없었나요?   
예를 들어 이미 소비된 메시지를 다시 보내게 되면 같은 데이터가 두 번 처리될 수 있는데, 이건 어떻게 방지하셨나요?
> 중복 처리를 별도로 막지는 않았습니다.
> 해당 토픽은 상품 이미지를 클라우드에 업로드하는 용도인데, 
> 사용자가 중간에 이미지를 수정했을 가능성도 있기 때문에, 오히려 나중에 처리되는 메시지가 더 정확한 상태를 반영할 수 있다고 판단했습니다.
> 즉, 이 프로세스는 idempotent할 필요가 없었고, 설계상 중복 처리 허용이 가능한 구조였습니다

### 🎯 6차 추가 질문
Kafka 시스템에서 가장 힘들었던 장애 상황이나 트러블슈팅 경험이 있으셨나요?   
예를 들어 컨슈머 지연, 메시지 누락, 브로커 이슈 등…
> Kafka 사용 초기에 메시지 유실을 제대로 대비하지 못한 상태로 서비스가 배포되어, 상점 측에서 클레임이 많이 들어왔던 경험이 있습니다.
> 초기에는 유실된 메시지를 로그와 DB 상태를 보고 직접 수동으로 재프로듀싱해서 처리했는데, 반복적으로 이슈가 발생하다 보니 결국
> 스케줄러를 통해 유실된 데이터를 매일 새벽 자동 탐지하고
> 관리자 UI에서 유실 메시지를 수동 재발행할 수 있는 기능을 개발해
> 운영 리스크를 줄이는 구조로 개선했습니다.

### 🎯 7차 Kafka 꼬리 질문
그 유실된 메시지를 탐지하는 로직은 어떻게 구현하셨나요?   
예를 들어 어떤 기준으로 “아, 이건 유실됐다”고 판단하셨는지 궁금합니다.
> Kafka 메시지가 발행되면 관련 테이블의 컬럼 값을 ‘진행중’으로 바꾸고, 컨슈머가 처리를 완료하면 ‘완료’로 상태를 갱신하는 구조였습니다.
> 대부분의 메시지는 10초 이내에 처리되지만, 하루가 지나도 ‘진행중’ 상태로 남아있는 경우는 유실된 것으로 판단하였습니다.
> 이 상태를 매일 새벽 스케줄러에서 체크하고, 유실된 것으로 추정되는 레코드가 발견되면 웹훅 알림을 발송하여
> 관리자가 UI에서 직접 해당 데이터를 다시 프로듀싱할 수 있도록 했습니다.

### 🎯 8차 Kafka 꼬리 질문 (심화)
컨슈머가 메시지를 소비는 했는데, 처리 중 오류가 나서 DB 업데이트가 되지 않은 경우도 있을 것 같은데요.   
이런 경우에는 중복 발행이나 불일치 문제가 발생할 수 있는데, 이건 어떻게 처리하셨나요?
> Kafka offset은 커밋하지 않으면 다음 메시지 lag이 쌓여서 전체 처리가 멈춰버리는 문제가 있습니다.
> 그래서 저는 메시지를 컨슘한 뒤 로직 중간에 실패가 발생하더라도 offset은 커밋하도록 구성했고,
> 그 대신 실패한 메시지는 별도의 리트라이 토픽에 발행해서 후속 처리를 이어갈 수 있도록 했습니다.
> 이렇게 하면 전체 처리 흐름이 멈추지 않고, 실패한 메시지도 따로 추적 및 복구할 수 있어
> 운영 측면에서 안정성이 더 높았습니다.

### 🎯 9차 Kafka 꼬리 질문
그 리트라이 토픽은 몇 번까지 재시도하도록 설계하셨나요?   
혹시 무한 재시도로 인해 리소스 낭비가 발생할 수 있는 구조는 아니었나요?
> 초기에는 리트라이 토픽에 재시도 횟수 제한을 두지 않아 무한 재시도가 발생했고,
> 그 결과 리소스 낭비와 모니터링 부담이 생기는 문제가 있었습니다.
> 그래서 정책적으로 최대 3회까지만 재시도하도록 설정했고,
> 3회 이상 실패할 경우에는 별도 웹훅 알림을 통해 메신저로 이슈를 수신했습니다.
> 이후 알림을 보고  코드 로직 문제면 로직을 수정해 재배포하고,
> 상점 이미지 자체에 문제가 있는 경우라면 담당자와 협의해 수정 요청을 보냈습니다.
> 이렇게 자동 처리 + 수동 모니터링 체계를 조합해 운영 안정성을 확보했습니다.

### 🎯 10차 Kafka 꼬리 질문
그럼 리트라이 횟수는 어떻게 카운팅하셨나요?   
메시지 안에 필드를 넣어서 관리하셨는지, 아니면 외부 저장소(DB, Redis 등)를 쓰셨는지 궁금합니다.
> 리트라이 횟수는 Kafka 메시지 안에 retryCount 필드를 추가해서 관리했습니다.
> 컨슈머에서 메시지를 처리 실패 시, 해당 필드를 +1 해서 다시 리트라이 토픽에 발행했고,
> 3회를 초과하면 해당 메시지는 더 이상 재처리하지 않고 웹훅 알림을 통해 수동 모니터링 대상으로 분리했습니다.
> 리트라이 횟수는 메시지 자체에 포함되어 있기 때문에 외부 저장소 없이도 간단히 추적 가능했고,
> 처리 로직도 복잡해지지 않아 운영 안정성에 효과적이었습니다.

## 2번 질문
ClickHouse를 사용하여 대규모 데이터 분석을 하셨다고 했는데, ClickHouse와 기존 RDBMS(MySQL 등)의 차이를 어떻게 체감하셨고, 어떤 상황에서 ClickHouse가 특히 강점을 발휘한다고 느끼셨나요?
> ClickHouse는 단순한 읽기/쓰기보다는 집계성 데이터를 빠르게 조회할 수 있도록 설계된 DB라, 특히 통계 리포팅 쪽에서 효과가 컸습니다.
> 컬럼 기반 저장 구조라 필요한 컬럼만 읽는 식으로 작동하다 보니, 기존 RDBMS 대비 속도 면에서 훨씬 나았고요.
> 이전 회사에서 리포팅 페이지가 많은 구조였는데, MySQL 기반으로는 복잡한 집계 쿼리들이 느려서 사용자 경험에 영향을 주는 상황이 있었습니다.
> 저는 배치 스케줄러를 활용해서, 당일 데이터를 제외한 나머지는 미리 집계한 데이터를 ClickHouse에 적재하는 방식으로 개선 작업을 진행했고, 
> 그 결과 응답 속도와 서버 부하 모두 개선되는 효과가 있었습니다.

### 🎯 1차 질문
ClickHouse를 실제 업무에 사용하셨다고 하셨는데, 당시 리포팅 성능 이슈는 구체적으로 어떤 상황에서 발생했나요?
> 통계데이터를 최대 13개월까지 한번에 보여줄수있었는데 
> (이전 회사는 따로 페이징이 없었고, 원페이지에 모든 데이터를 표현해야하는 사이트였습니다.)
> 많은 양의 일별, 주별, 월별 데이터를 한 페이지에 보여줘야하다보니깐 브라우저 메모리 이슈라든가, 
> 페이지가 1분 이상 로딩되어 사용자에게 불편함을 주는 경우가 발생하였습니다.

### 🎯 2차 꼬리 질문
그 이슈를 해결하기 위해 ClickHouse에 어떤 구조로 데이터를 적재했고, 배치 스케줄러는 어떤 방식으로 구성하셨나요?    
예를 들어 집계 주기, 스케줄링 도구, 쿼리 방식 같은 게 궁금합니다.
> 통계데이터의 특징을 활용하였습니다. 통계데이터는 당일자가 아니면 데이터가 변하거나 추가되는 경우가 있어서는 안됩니다.(사용자의 돈과 관련되어있기 떄문에)
> 그래서 당일자는 실시간으로 보여주되, 나머지 일자에 해당하는 데이터는 스케줄러를 통해서 clickhouse 미리 집계성 데이터를 만들어놔서 사용되도록하였습니다.
> 스케줄러는 사용자가 제일 사용량이 적은 매일 새벽에 돌아가도록 하였는데 
> 스케줄링 도구는 스프링 배치를 이용하였고 쿼리 방식은 기존 리포팅 페이지에 사용하던 쿼리를 그대로 사용하여 clickhouse에 재적재하는 방식으로 작업하였습니다.

### 🎯 3차 꼬리 질문
ClickHouse에 데이터를 재적재했다고 하셨는데, 혹시 중복 적재나 데이터 무결성 문제는 어떻게 방지하셨나요?    
예를 들어, 배치가 실패하거나 두 번 실행될 경우 문제가 생기지는 않았는지 궁금합니다.
> 그런 경우가 많이 발생하지 않았지만, 리포팅 페이지에서 해당 날짜에 해당하는 클릭하우스 데이터가 없는 경우 
> Rdbms(원본 데이터)에서 해당 날짜 데이터를 가져오도록 예외처리를 해놓았습니다.
> 그래서 만약 배치가 실패하거나 두번 실행되어 데이터 무결성문제가 발생하는 경우 
> 클릭하우스에 적재된 문제된 데이터를 다 삭제하고 다시 배치를 수동으로 실행하는 방법으로 이슈를 해결하였습니다.

### 🎯 4차 꼬리 질문
RDBMS에서 데이터를 가져올 때 성능 이슈는 없었나요?   
예외 상황이라 하더라도 리포팅 페이지에서 느려지거나 장애로 이어지는 리스크는 없었는지, 그에 대한 대응은 어떻게 하셨는지도 궁금합니다.
> 맞습니다, 예외 상황에서는 실제로 리포팅 페이지가 느려질 수밖에 없었습니다.
> 이런 경우에는 빠르게 ClickHouse에 재적재를 진행하는 게 가장 현실적인 해결 방법이었고, 
> 실제로 그런 상황이 발생하면 바로 수동으로 배치를 돌려서 사용자 불편을 최소화하려고 했습니다.
> 물론 장기적으로는 fallback 조회도 비동기적으로 처리하거나, 
> 사용자에게 '데이터 갱신 중입니다' 같은 메시지를 보여주는 방식도 고려는 했는데, 
> 당시엔 빠른 수동 재처리가 가장 효율적인 방법이었습니다.

### 🎯 5차 마지막 꼬리 질문
ClickHouse 적재 시 데이터 구조는 어떤 식으로 설계하셨나요?
예를 들어 날짜별 파티셔닝이나 MergeTree 엔진 등 어떤 설정을 활용하셨는지도 궁금합니다.
> ClickHouse에 집계성 데이터를 넣을 때는 리포팅 기준으로 자주 조회하는 컬럼 위주로 모델링했습니다.
> 엔진은 기본 MergeTree를 사용했고, shop_id, report_date 기준으로 ORDER BY 설정, 그리고 월 단위 toYYYYMM(report_date)로 파티셔닝해서 성능을 높였습니다.
> TTL 설정은 따로 하지는 않았지만, 장기적으로는 13개월 넘은 데이터는 자동 삭제하도록 구성하는 것도 고려할 수 있다고 생각합니다.

## 3번 질문
다양한 언어(PHP, Java, Kotlin, Python 등)를 사용해 오셨는데, 언어 선택 기준은 무엇이고,    
팀 내에서 새로운 언어 도입 시 어떻게 의견을 조율하셨는지 궁금합니다.
> 다양한 언어는 회사의 기술 스택 또는 프로젝트 성격에 따라 자연스럽게 접하게 되었습니다. 
> 예를 들어, PHP는 기존 레거시 시스템 유지보수나 간단한 웹페이지 구축에, 
> Java나 Kotlin은 복잡한 백엔드 서비스에 적합해서 사용되었고, 
> Python은 데이터 처리나 배치 작업 등 유틸성 스크립트에 주로 활용되었습니다.
> 언어를 제가 직접 선택한 건 아니었지만, 새로운 언어나 프레임워크가 도입될 때마다 빠르게 적응하고, 
> 팀원들과 지식 공유를 통해 함께 성장할 수 있도록 노력했습니다. 
> 새로운 언어 도입에 있어서는 “우리 커리어에 도움이 될 뿐만 아니라, 
> 유지보수성과 개발 효율성 측면에서도 의미 있는 선택”이라는 점을 공감대로 만들려 노력했습니다.

## 4번 질문
성능 개선을 위해 캐시 서버(Redis)를 도입하신 경험이 있다고 하셨는데,   
어떤 문제를 해결하기 위해 캐시를 적용하셨고, 적용 후 실제로 어떤 변화가 있었는지 구체적으로 설명해주실 수 있나요?
> 통계 데이터를 시각화해주는 리포팅 페이지에서 성능 이슈가 심각했습니다.
> 예를 들어, 일/주/월 단위의 통계 데이터를 최대 13개월까지 한번에 불러오는 구조였고, 쿼리량이 많아 브라우저 로딩 속도가 1분 이상 걸리는 경우도 있었습니다. 
> 특히, 비즈니스적으로 사용자 경험이 중요한 페이지였기 때문에 속도 개선이 시급했습니다.
> ClickHouse를 도입하여 집계 성능 자체는 개선되었지만, 동일한 요청이 반복되는 경우가 많아 추가적으로 Redis를 도입해 자주 조회되는 통계 데이터를 캐싱하는 방식으로 접근했습니다.
> Redis에서는 요청 조건에 따라 생성된 응답 JSON을 key-value 형태로 저장했고, 
> key는 통계타입:기간:상점번호 등으로 구성해 캐시 구분이 명확하게 되도록 했습니다.
> 데이터 갱신은 두 가지 방식으로 처리했습니다.
> 당일 데이터는 실시간이기 때문에 캐시를 사용하지 않고 매번 조회
> 이전 날짜 데이터는 변경될 일이 없기 때문에 스케줄러를 통해 캐싱 후 일정 시간 또는 수동 갱신
> 결과적으로, 캐시를 도입한 후 1분 이상 걸리던 페이지 로딩 속도가 1초 이내로 감소했고, 클라이언트 및 비즈니스 측 피드백도 매우 긍정적이었습니다.